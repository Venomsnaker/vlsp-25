{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12786233,"sourceType":"datasetVersion","datasetId":8080717}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Set Up","metadata":{}},{"cell_type":"code","source":"!pip install unsloth vllm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"from unsloth import FastVisionModel\nimport torch\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n    load_in_4bit = True,\n    use_gradient_checkpointing = \"unsloth\", \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lora_rank = 8\n\nmodel = FastVisionModel.get_peft_model(\n    model,\n    finetune_vision_layers     = True, \n    finetune_language_layers   = True, \n    finetune_attention_modules = True,\n    finetune_mlp_modules       = True, \n\n    r = lora_rank,          \n    lora_alpha = lora_rank*2, \n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n    use_rslora = False, \n    loftq_config = None, \n    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"import json\n\ntrain_path = \"/kaggle/input/vlsp-2025-dataset/train_data/vlsp_2025_train.json\"\n\nwith open(train_path, \"r\", encoding=\"utf-8\") as f:\n    train = json.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nfrom datasets import Dataset\n\nsystem_prompt=(\n    \"Bạn là một trợ lý pháp lý tiếng Việt chuyên phân tích câu hỏi liên quan đến luật giao thông đường bộ và biển báo. \"\n    \"Luôn trả lời hoàn toàn bằng tiếng Việt.\\n\\n\"\n    \"Trả lời theo định dạng sau:\\n\"\n    \"<think>\"\n    \"[Suy nghĩ, phân tích của bạn]\"\n    \"</think>\"\n    \"[Câu trả lời của bạn]\"\n)\n\ndef create_conversation(input):\n    img_path = \"/kaggle/input/vlsp-2025-dataset/train_data/train_images/\" + input[\"image_id\"] + \".jpg\"\n    img = Image.open(img_path)\n\n    if input[\"question_type\"] == \"Multiple choice\":\n        if input[\"answer\"] == 40:  # Process edge case\n            input[\"answer\"] = \"A\"\n\n        user_prompt = (\n            \"Dựa vào bối cảnh bên dưới, hãy phân tích kỹ trước khi trả lời câu hỏi.\\n\\n\"\n            \"Loại câu hỏi: Trắc nghiệm (kết luận cuối cùng sau khi suy luận là 1 trong 4 lựa chọn: ‘A’, ‘B’, ‘C’, ‘D’. Không được giải thích gì thêm.)\\n\\n\"\n            f\"Câu hỏi: {input['question']}\\n\\n\"\n            \"4 lựa chọn:\\n\"\n            f\"A: {input['choices']['A']}\\n\"\n            f\"B: {input['choices']['B']}\\n\"\n            f\"C: {input['choices']['C']}\\n\"\n            f\"D: {input['choices']['D']}\\n\\n\"\n            \"Hãy trả lời theo định dạng:\\n\"\n            \"<think>…phân tích chi tiết…</think><answer>A/B/C/D</answer>\"\n        )\n    else:\n        user_prompt = (\n            \"Dựa vào bức ảnh, hãy phân tích kỹ trước khi trả lời câu hỏi.\\n\\n\"\n            \"Loại câu hỏi: Đúng/Sai (kết luận cuối cùng chỉ là một từ: 'Đúng' hoặc 'Sai'. Không được giải thích gì thêm.)\\n\\n\"\n            f\"Câu hỏi: {input['question']}\\n\\n\"\n            \"Hãy trả lời theo định dạng:\\n\"\n            \"<think>…phân tích chi tiết…</think><answer>Đúng/Sai</answer>\"\n        )\n    return [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\",  \"text\": user_prompt},\n                {\"type\": \"image\", \"image\": img}\n            ],\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": input[\"answer_think\"]}\n            ],\n        },\n    ]\n\ndef convert_input_to_grpo(input):\n    conversation = create_conversation(input)\n    rendered_prompt = tokenizer.apply_chat_template(\n        conversation,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    return {\n        \"prompt\": rendered_prompt,\n        \"answer\": str(input[\"answer\"]),\n    }\n\ntrain_grpo = [convert_input_to_grpo(x) for x in train]\ntrain_ds = Dataset.from_list(train_grpo)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Reward","metadata":{}},{"cell_type":"code","source":"import re\nfrom typing import Dict, Any\n\n_ANSWER_TAG_RE = re.compile(r\"<answer>\\s*([\\s\\S]+?)\\s*</answer>\", flags=re.IGNORECASE)\n\ndef _extract_answer(text: str) -> str:\n    if not text:\n        return \"\"\n    m = _ANSWER_TAG_RE.search(text)\n    if m:\n        return m.group(1).strip()\n    return text.strip().splitlines()[-1].strip()\n\ndef reward_function(prompts, completions, **kwargs):\n    rewards = []\n    \n    for prompt, pred_text in zip(prompts, completions):\n        score = 0.0\n\n        # Has good formatting\n        w_formatting = 1\n        \n        has_think = \"<think>\" in pred_text and \"</think>\" in pred_text\n        has_answer = \"<answer>\" in pred_text and \"</answer>\" in pred_text\n        score += (w_formatting / 2) if has_think else 0\n        score += (w_formatting / 2) if has_answer else 0\n\n        # Contain Vietnamese characters\n        w_vietnamese = 0.5\n        \n        vietnamese_chars = \"ăâđêôơưáàảãạéèẻẽẹóòỏõọúùủũụíìỉĩị\"\n        vn_count = sum(c in vietnamese_chars for c in pred_text.lower())\n        if vn_count >= 10:\n            score += w_vietnamese\n\n        # Reasoning quality\n        w_reasoning = 3.5\n        \n        if has_think:\n            think_content = pred_text.split(\"<think>\")[1].split(\"</think>\")[0].strip()\n            \n            # Adequate length — encourage more developed reasoning\n            length_bonus = min(len(think_content.split()) / 100, 1.0) \n            score += w_reasoning * 0.3 * length_bonus\n\n            # Keyword bonus — reward structured analysis reasoning\n            reasoning_keywords = [\n                \"phân tích\", \"giải thích\", \"so sánh\", \"đối chiếu\",\n                \"nguyên nhân\", \"kết luận\", \"lý do\", \"bước\", \"giả thiết\"\n            ]\n            keyword_hits = sum(kw in think_content.lower() for kw in reasoning_keywords)\n            if keyword_hits > 0:\n                score += w_reasoning * 0.4 * min(keyword_hits / 3, 1.0)\n\n            # Completeness check — did it consider all answer choices A–D?/ evaluation words (Đúng, Sai)\n            multiple_choices_responses = [\"a\", \"b\", \"c\", \"d\"]\n            yes_no_responses = [\"đúng\", \"sai\"]\n\n            mc_point = 0.25\n            yn_point = 0.5\n            \n            choices_score = sum(\n                mc_point for ch in multiple_choices_responses \n                if re.search(rf\"\\b{ch}\\b\", think_content.lower())\n            )\n            yes_no_score = sum(\n                yn_point for word in yes_no_responses \n                if word in think_content.lower()\n            )\n            score += w_reasoning * max(choices_score, yes_no_score)\n\n        # Correctness\n        w_correctness = 5\n        \n        pred_ans = _extract_answer(pred_text)\n        true_ans = kwargs.get(\"answers\", [\"\"] * len(prompts))[len(rewards)]\n        if pred_ans.strip().lower() == str(true_ans).strip().lower():\n            score += w_correctness\n        \n        rewards.append(score)\n    return rewards\n\ndef reward_wrapper(prompts, completions, **kwargs):\n    answers = kwargs.get(\"samples\", {}).get(\"answer\", [\"\"] * len(prompts))\n    return reward_function(prompts, completions, answers=answers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"from trl import GRPOTrainer, GRPOConfig\n\ngrpo_config = GRPOConfig(\n    output_dir=\"outputs\",\n    num_generations=4,     \n    learning_rate=2e-4,\n    max_completion_length=2048,\n    temperature=0.7,\n    beta=0.1,\n    num_train_epochs = 1,                 \n    gradient_accumulation_steps=4,\n    fp16=True,\n    logging_steps=1,\n    save_steps=10,\n    save_total_limit = 3,\n    seed = 3407,\n    report_to=[\"none\"],\n)\n\ngrpo_config_test = GRPOConfig(\n    output_dir=\"outputs\",\n    learning_rate=1e-5,\n    max_completion_length=2048,\n    beta=0.1,\n    num_generations=2,\n    temperature=0.7,\n    logging_steps=1,\n    save_steps=5,\n    max_steps=10,\n    gradient_accumulation_steps=1,\n    fp16=True,\n    report_to=[\"none\"],\n)\n\n\ntrainer = GRPOTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    reward_funcs=reward_wrapper,\n    args=grpo_config,\n    train_dataset=train_ds,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import HfApi\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nrepo = \"venomsnaker/Qwen2.5-VL-7B-Instruct-GRPO\"\n\nsave_dir = \"outputs/final_model\"\ntrainer.save_model(save_dir)  \ntokenizer.save_pretrained(save_dir)\n\nmodel.push_to_hub_merged(repo, tokenizer, save_method = \"merged_16bit\", token = hf_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}